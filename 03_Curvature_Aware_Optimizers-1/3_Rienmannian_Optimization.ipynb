{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7c6ba6",
   "metadata": {},
   "source": [
    "# üåê Riemannian Optimization ‚Äî Optimization on Manifolds\n",
    "\n",
    "> ‚ÄúOptimization constrained to a curved space ‚Äî not by projections, but by geometry.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objectives\n",
    "In this notebook we explore **Riemannian Gradient Descent (RGD)** ‚Äî a generalization of gradient descent to *manifolds* (curved spaces).  \n",
    "We will:\n",
    "\n",
    "- Build intuition for optimization *on curved spaces* (e.g., spheres, Stiefel manifolds).  \n",
    "- Derive the **Riemannian gradient** and **retraction** mechanism.  \n",
    "- Implement RGD on:\n",
    "  1. The **unit sphere** (constraint: ‚Äñx‚Äñ = 1)\n",
    "  2. The **Stiefel manifold** (orthogonal matrix constraints)\n",
    "- Visualize **Euclidean vs Riemannian** steps on 3D surfaces.  \n",
    "- Interactively tune learning rate and visualize convergence along geodesics.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© References\n",
    "- Absil, Mahony, Sepulchre (2008): *Optimization Algorithms on Matrix Manifolds*  \n",
    "- B√©cigneul & Ganea (2019): *Riemannian Adaptive Optimization Methods* (ICLR)  \n",
    "- Amari (1998): *Natural Gradient Works Efficiently in Learning*  \n",
    "- Edelman et al. (1998): *The Geometry of Algorithms with Orthogonality Constraints*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6dd4b8",
   "metadata": {},
   "source": [
    "## üí° What is Riemannian Optimization?\n",
    "\n",
    "In standard (Euclidean) gradient descent, we minimize $ f(x) $ in $ \\mathbb{R}^n $:\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\nabla f(x_t)\n",
    "$$\n",
    "\n",
    "But what if $ x $ must **stay on a manifold**, e.g. the sphere $ S^{n-1} = \\{ x : \\|x\\| = 1 \\} $?\n",
    "\n",
    "Then:\n",
    "1. The **tangent space** $ T_x\\mathcal{M} $ replaces the ambient space.  \n",
    "2. The **Riemannian gradient** is the projection of the Euclidean gradient onto $ T_x\\mathcal{M} $.  \n",
    "3. The update uses a **retraction** (or exponential map) to move along the manifold.\n",
    "\n",
    "$$\n",
    "x_{t+1} = \\text{Retr}_x(-\\eta \\, \\textit{grad}_x f)\n",
    "$$\n",
    "\n",
    "- The **Riemannian gradient**: $ \\text{grad}_x f = P_{T_x}(\\nabla f) $\n",
    "- The **retraction**: maps tangent vector back to the manifold  \n",
    "  - On the sphere: $ \\text{Retr}_x(v) = \\frac{x + v}{\\|x + v\\|} $\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Example: Sphere constraint\n",
    "$$\n",
    "\\min_{\\|x\\|=1} f(x) = x^\\top A x\n",
    "$$\n",
    "- Euclidean gradient: $ \\nabla f = 2 A x $\n",
    "- Riemannian gradient: $ \\text{grad}_x f = \\nabla f - (x^\\top \\nabla f) x $ \n",
    "\n",
    "  (the component of ‚àáf tangent to the sphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3fcf6",
   "metadata": {},
   "source": [
    "### Step-1: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da3564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from IPython.display import clear_output, display, Markdown\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, FloatSlider, IntSlider\n",
    "except ImportError:\n",
    "    !pip install ipywidgets\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, FloatSlider, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323217a5",
   "metadata": {},
   "source": [
    "### Step-2: Riemannian Gradient Descent on Sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f19bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def riemannian_gd_sphere(A, x0, lr=0.1, n_iter=50):\n",
    "    x = x0 / np.linalg.norm(x0)\n",
    "    path = [x]\n",
    "    losses = []\n",
    "    for t in range(n_iter):\n",
    "        g_euc = sphere_grad(x, A)\n",
    "        g_riem = proj_tangent_sphere(x, g_euc)\n",
    "        x = retr_sphere(x, -lr * g_riem)\n",
    "        path.append(x)\n",
    "        losses.append(sphere_cost(x, A))\n",
    "    return np.array(path), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b7254",
   "metadata": {},
   "source": [
    "### Step-3: Helper Function for Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df17c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_tangent_sphere(x, g):\n",
    "    \"\"\"Project Euclidean gradient g onto tangent space of unit sphere at x.\"\"\"\n",
    "    return g - np.dot(x, g) * x\n",
    "\n",
    "def retr_sphere(x, v):\n",
    "    \"\"\"Retract tangent vector v back onto sphere.\"\"\"\n",
    "    y = x + v\n",
    "    return y / np.linalg.norm(y)\n",
    "\n",
    "def sphere_cost(x, A):\n",
    "    \"\"\"Quadratic cost on sphere: f(x) = x^T A x\"\"\"\n",
    "    return x.T @ A @ x\n",
    "\n",
    "def sphere_grad(x, A):\n",
    "    \"\"\"Euclidean gradient.\"\"\"\n",
    "    return 2 * A @ x\n",
    "\n",
    "def plot_sphere(A, path=None, title=\"Riemannian GD on Sphere\"):\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    u = np.linspace(0, 2*np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    xs = np.outer(np.cos(u), np.sin(v))\n",
    "    ys = np.outer(np.sin(u), np.sin(v))\n",
    "    zs = np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "    ax.plot_surface(xs, ys, zs, color='lightgray', alpha=0.3, linewidth=0)\n",
    "    if path is not None:\n",
    "        ax.plot(path[:,0], path[:,1], path[:,2], 'r-', lw=2, label='trajectory')\n",
    "        ax.scatter(path[0,0], path[0,1], path[0,2], c='blue', s=60, label='start')\n",
    "        ax.scatter(path[-1,0], path[-1,1], path[-1,2], c='black', marker='*', s=100, label='end')\n",
    "    ax.set_title(title)\n",
    "    ax.set_box_aspect([1,1,1])\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3834f4",
   "metadata": {},
   "source": [
    "### Step-4: Interactive Riemannian GD on Sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0bb10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e118bd6a7e94e3c9267016405d76d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='lr', max=1.0, min=0.01, step=0.01), IntSlider(value=‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_sphere_rgd(lr=0.2, n_iter=50)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interactive_sphere_rgd(lr=0.2, n_iter=50):\n",
    "    clear_output(wait=True)\n",
    "    A = np.diag([1.0, 0.5, -0.2]) + 0.1*np.random.randn(3,3)\n",
    "    A = (A + A.T)/2\n",
    "    x0 = np.random.randn(3)\n",
    "    path, losses = riemannian_gd_sphere(A, x0, lr=lr, n_iter=n_iter)\n",
    "    display(Markdown(f\"### Riemannian Gradient Descent on Sphere\\nLearning rate: {lr}, Iterations: {n_iter}\"))\n",
    "    plot_sphere(A, path)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Iteration\"); plt.ylabel(\"Loss\"); plt.title(\"Loss vs Iteration\"); plt.grid(True, ls='--', lw=0.4)\n",
    "    plt.show()\n",
    "\n",
    "interact(interactive_sphere_rgd,\n",
    "         lr=FloatSlider(value=0.2, min=0.01, max=1.0, step=0.01, description='lr'),\n",
    "         n_iter=IntSlider(value=50, min=10, max=200, step=5, description='n_iter'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6511d79a",
   "metadata": {},
   "source": [
    "## üßÆ The Stiefel Manifold $ \\text{St}(p, n) $\n",
    "\n",
    "$$\n",
    "\\text{St}(p, n) = \\{ X \\in \\mathbb{R}^{n \\times p} \\ | \\ X^\\top X = I_p \\}\n",
    "$$\n",
    "\n",
    "Each point $ X $ has orthonormal columns ‚Äî e.g., a subspace basis.\n",
    "\n",
    "### Tangent Space\n",
    "At a point $ X $, the tangent space is:\n",
    "$$\n",
    "T_X = \\{ Z : X^\\top Z + Z^\\top X = 0 \\}\n",
    "$$\n",
    "\n",
    "### Riemannian Gradient\n",
    "The projection of the Euclidean gradient $ G $ onto $ T_X $ is:\n",
    "$$\n",
    "\\text{grad}_X f = G - X \\cdot \\text{sym}(X^\\top G)\n",
    "$$\n",
    "where $ \\text{sym}(M) = \\tfrac{1}{2}(M + M^\\top) $.\n",
    "\n",
    "### Retraction (QR-based)\n",
    "To move along tangent directions while keeping orthonormality:\n",
    "$$\n",
    "\\text{Retr}_X(V) = \\text{qf}(X + V)\n",
    "$$\n",
    "where `qf` denotes the Q factor of the QR decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7eefa",
   "metadata": {},
   "source": [
    "### Step-5: Stiefel Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185fd441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_tangent_stiefel(X, G):\n",
    "    \"\"\"Project Euclidean gradient G onto tangent space of Stiefel manifold.\"\"\"\n",
    "    return G - X @ ((X.T @ G + G.T @ X)/2)\n",
    "\n",
    "def retr_stiefel(X, V):\n",
    "    \"\"\"Retraction via QR decomposition.\"\"\"\n",
    "    Y, _ = np.linalg.qr(X + V)\n",
    "    return Y\n",
    "\n",
    "def stiefel_cost(X, A):\n",
    "    return np.trace(X.T @ A @ X)\n",
    "\n",
    "def stiefel_grad(X, A):\n",
    "    return 2 * A @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98aacb4",
   "metadata": {},
   "source": [
    "### Step-6: RGD on Stiefel Manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f63012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def riemannian_gd_stiefel(A, X0, lr=0.1, n_iter=50):\n",
    "    X = X0.copy()\n",
    "    path = [X]\n",
    "    losses = []\n",
    "    for t in range(n_iter):\n",
    "        G_euc = stiefel_grad(X, A)\n",
    "        G_riem = proj_tangent_stiefel(X, G_euc)\n",
    "        X = retr_stiefel(X, -lr * G_riem)\n",
    "        losses.append(stiefel_cost(X, A))\n",
    "        path.append(X)\n",
    "    return path, np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9306e",
   "metadata": {},
   "source": [
    "### Step-7: Interactive Stiefel Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ebdc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6787b6090f414d97abe47f5e849011d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='lr', max=0.5, min=0.01, step=0.01), IntSlider(value=‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_stiefel_rgd(lr=0.1, n_iter=50, n=3, p=2)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interactive_stiefel_rgd(lr=0.1, n_iter=50, n=3, p=2):\n",
    "    clear_output(wait=True)\n",
    "    A = np.random.randn(n,n)\n",
    "    A = (A + A.T)/2\n",
    "    X0, _ = np.linalg.qr(np.random.randn(n,p))\n",
    "    path, losses = riemannian_gd_stiefel(A, X0, lr=lr, n_iter=n_iter)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(losses)\n",
    "    plt.title(f\"Riemannian GD on St({p},{n}) - Loss vs Iteration\")\n",
    "    plt.xlabel(\"Iteration\"); plt.ylabel(\"Loss\"); plt.grid(True)\n",
    "    plt.show()\n",
    "    display(Markdown(f\"Final orthogonality check: X·µÄX ‚âà I ‚Äî deviation: {np.linalg.norm(path[-1].T@path[-1]-np.eye(p)):.2e}\"))\n",
    "\n",
    "interact(interactive_stiefel_rgd,\n",
    "         lr=FloatSlider(value=0.1, min=0.01, max=0.5, step=0.01, description='lr'),\n",
    "         n_iter=IntSlider(value=50, min=10, max=200, step=5, description='n_iter'),\n",
    "         n=IntSlider(value=3, min=2, max=5, step=1, description='ambient n'),\n",
    "         p=IntSlider(value=2, min=1, max=3, step=1, description='cols p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4c988",
   "metadata": {},
   "source": [
    "## üîç Key Takeaways\n",
    "\n",
    "| Concept | Euclidean | Riemannian |\n",
    "|----------|------------|------------|\n",
    "| Space | $ \\mathbb{R}^n $ | Manifold $ \\mathcal{M} $ |\n",
    "| Gradient | $ \\nabla f $ | Projection onto tangent space $ \\text{grad}_x f $ |\n",
    "| Update | $ x - \\eta \\nabla f $ | $ \\text{Retr}_x(-\\eta \\, \\text{grad}_x f) $ |\n",
    "| Movement | Straight line | Geodesic curve |\n",
    "| Constraint enforcement | Projection after update | Intrinsic movement (always on manifold) |\n",
    "\n",
    "---\n",
    "\n",
    "### üåê Relation to Natural Gradient\n",
    "The **natural gradient** is a special case of Riemannian gradient descent where the manifold is the *statistical manifold* with metric given by the Fisher information matrix.  \n",
    "Thus, Riemannian optimization generalizes natural gradient descent beyond probability models.  \n",
    "\n",
    "$$\n",
    "\\text{NGD: } \\tilde{\\nabla}_\\theta = F^{-1} \\nabla_\\theta f\n",
    "\\quad \\text{is just Riemannian GD on the Fisher manifold.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36423ac1",
   "metadata": {},
   "source": [
    "## üìö References\n",
    "\n",
    "- Absil, P.-A., Mahony, R., Sepulchre, R. (2008). *Optimization Algorithms on Matrix Manifolds.* Princeton University Press.  \n",
    "- Edelman, A., Arias, T.A., & Smith, S.T. (1998). *The Geometry of Algorithms with Orthogonality Constraints.* SIAM J. Matrix Anal. Appl.  \n",
    "- B√©cigneul, G., & Ganea, O.-E. (2019). *Riemannian Adaptive Optimization Methods.* ICLR.  \n",
    "- Amari, S. (1998). *Natural Gradient Works Efficiently in Learning.* Neural Computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
