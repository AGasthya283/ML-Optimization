{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a2d309",
   "metadata": {},
   "source": [
    "# ðŸ”¶ K-FAC from Scratch â€” Layerwise Kronecker Factors, Implementation & Interactive Comparison\n",
    "\n",
    "**What this notebook does**\n",
    "\n",
    "- Implements a small fully connected neural network (one hidden layer) in NumPy.\n",
    "- Implements three optimizers:\n",
    "  - SGD (vanilla)\n",
    "  - Adam (NumPy)\n",
    "  - **K-FAC**: **Kronecker-factored Approximate Curvature** (layerwise), with damping and periodic factor updates.\n",
    "- Trains on a synthetic 2-class dataset and visualizes:\n",
    "  - Loss and accuracy vs iterations\n",
    "  - Per-layer effective preconditioned step norms\n",
    "  - Parameter trajectories projected into a 2D PCA subspace (visual diagnostic)\n",
    "- Adds interactive controls (learning rate, damping, K-FAC update frequency, batch size, optimizer selection).\n",
    "\n",
    "**Pedagogical notes**\n",
    "\n",
    "- The K-FAC implementation here is simplified and focused on clarity: it demonstrates how to collect layerwise Kronecker factors (covariances of activations and gradients), form damped inverses, and precondition the parameter gradients using the identity\n",
    "$$\n",
    "\\mathrm{vec}(\\Delta W) \\approx (G^{-1} \\otimes A^{-1}) \\, \\mathrm{vec}(\\nabla_W)\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\Delta W \\approx A^{-1} \\, \\nabla_W \\, G^{-1}\n",
    "$$\n",
    "for a dense layer with weight matrix $W$, activation covariance $A$, and gradient/output covariance $G$. See Martens & Grosse (2015). \n",
    "\n",
    "**References**\n",
    "- Martens, J., & Grosse, R. (2015). *Optimizing Neural Networks with Kronecker-Factored Approximate Curvature (K-FAC)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef40bd3",
   "metadata": {},
   "source": [
    "### Step-1: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370f1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from ipywidgets import interact, FloatSlider, IntSlider, Dropdown, Checkbox\n",
    "except Exception:\n",
    "    # if ipywidgets not installed, the notebook will still run static examples\n",
    "    widgets = None\n",
    "    interact = None\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10923269",
   "metadata": {},
   "source": [
    "### Step-2: Setting up a Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8762d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_two_moons(n_samples=200, noise=0.15):\n",
    "    # a compact two-moons generator (numpy)\n",
    "    n = n_samples // 2\n",
    "    theta = np.linspace(0, np.pi, n)\n",
    "    x1 = np.stack([np.cos(theta), np.sin(theta)], axis=1) + noise * np.random.randn(n,2)\n",
    "    x2 = np.stack([1 - np.cos(theta), -np.sin(theta) - 0.5], axis=1) + noise * np.random.randn(n,2)\n",
    "    X = np.vstack([x1, x2])\n",
    "    y = np.hstack([np.zeros(n, dtype=int), np.ones(n, dtype=int)])\n",
    "    # shuffle\n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "X, y = make_two_moons(n_samples=400, noise=0.12)\n",
    "# simple one-hot labels\n",
    "Y = np.eye(2)[y]\n",
    "\n",
    "# train / val split\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, Y_train = X[:split], Y[:split]\n",
    "X_val, Y_val = X[split:], Y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6c02a",
   "metadata": {},
   "source": [
    "### Step-3: Defining a Small MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f68f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(logits):\n",
    "    ex = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    return ex / ex.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(probs, targets):\n",
    "    # targets: one-hot\n",
    "    eps = 1e-12\n",
    "    return -np.mean(np.sum(targets * np.log(probs + eps), axis=1))\n",
    "\n",
    "class SmallMLP:\n",
    "    def __init__(self, d_in=2, d_hidden=32, d_out=2, scale=0.1):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = scale * np.random.randn(d_in, d_hidden)\n",
    "        self.params['b1'] = np.zeros(d_hidden)\n",
    "        self.params['W2'] = scale * np.random.randn(d_hidden, d_out)\n",
    "        self.params['b2'] = np.zeros(d_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "        a1 = relu(z1)\n",
    "        z2 = a1.dot(self.params['W2']) + self.params['b2']\n",
    "        probs = softmax(z2)\n",
    "        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'probs': probs}\n",
    "        return probs, cache\n",
    "\n",
    "    def loss_and_grads(self, X, Y):\n",
    "        probs, cache = self.forward(X)\n",
    "        loss = cross_entropy_loss(probs, Y)\n",
    "        # grads by backprop (batch)\n",
    "        N = X.shape[0]\n",
    "        dz2 = (probs - Y) / N  # dL/dz2\n",
    "        dW2 = cache['a1'].T.dot(dz2)\n",
    "        db2 = dz2.sum(axis=0)\n",
    "        da1 = dz2.dot(self.params['W2'].T)\n",
    "        dz1 = da1 * relu_grad(cache['z1'])\n",
    "        dW1 = X.T.dot(dz1)\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "        return loss, grads, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba909a",
   "metadata": {},
   "source": [
    "### Step-4: Prepping Bseline Optimizers for comparison (SGD, Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef83987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, params, lr=0.1, weight_decay=0.0):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.wd = weight_decay\n",
    "\n",
    "    def step(self, grads):\n",
    "        for k, g in grads.items():\n",
    "            self.params[k] -= self.lr * (g + self.wd * self.params[k])\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.wd = weight_decay\n",
    "        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, grads):\n",
    "        self.t += 1\n",
    "        for k, g in grads.items():\n",
    "            self.m[k] = self.beta1 * self.m[k] + (1 - self.beta1) * g\n",
    "            self.v[k] = self.beta2 * self.v[k] + (1 - self.beta2) * (g * g)\n",
    "            m_hat = self.m[k] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[k] / (1 - self.beta2**self.t)\n",
    "            self.params[k] -= self.lr * (m_hat / (np.sqrt(v_hat) + self.eps) + self.wd * self.params[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb5258",
   "metadata": {},
   "source": [
    "### Step-5: Defining Helper Funstions for K-FAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1450e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance(mat, eps=1e-6):\n",
    "    # mat: (batch, dim) with row samples; return empirical covariance E[xx^T]\n",
    "    # we compute uncentered covariance for K-FAC (activations typically with bias appended externally)\n",
    "    cov = (mat.T @ mat) / mat.shape[0]\n",
    "    # numerical stability: add tiny eps to diagonal\n",
    "    cov += eps * np.eye(cov.shape[0])\n",
    "    return cov\n",
    "\n",
    "def damp_and_invert(mat, damping):\n",
    "    # mat: symmetric positive (approx). returns inverse of (mat + damping * I)\n",
    "    m = mat + damping * np.eye(mat.shape[0])\n",
    "    try:\n",
    "        inv = np.linalg.inv(m)\n",
    "    except np.linalg.LinAlgError:\n",
    "        inv = np.linalg.pinv(m)\n",
    "    return inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deedac6b",
   "metadata": {},
   "source": [
    "### Step-6: Implementing K-FAC Optimizer (Layerwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597ebd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACOptimizer:\n",
    "    def __init__(self, model, lr=0.1, damping=1e-3, kl_clip=0.001, factor_ema=0.95, invert_every=10, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.damping = damping\n",
    "        self.kl_clip = kl_clip\n",
    "        self.factor_ema = factor_ema\n",
    "        self.invert_every = invert_every\n",
    "        self.weight_decay = weight_decay\n",
    "        # storage for factors and inverses\n",
    "        self.A = {}  # activation covariances\n",
    "        self.G = {}  # gradient covariances\n",
    "        self.A_inv = {}\n",
    "        self.G_inv = {}\n",
    "        self.steps = 0\n",
    "\n",
    "    def _extract_activations_and_errors(self, cache, grads):\n",
    "        # For our 2-layer MLP:\n",
    "        # layer1: W1 weight shape (d_in, d_hidden); activations are X (batch, d_in)\n",
    "        # layer2: W2 weight shape (d_hidden, d_out); activations are a1 (batch, d_hidden)\n",
    "        activations = {'W1': cache['X'], 'W2': cache['a1']}\n",
    "        errors = {'W1': grads['W1'], 'W2': grads['W2']}  # these are full-matrix weight gradients (d_in x d_hidden etc.)\n",
    "        # For the gradient covariances G we use the gradients w.r.t. pre-activations of outputs (dz)\n",
    "        # Here errors above are dW = A^T @ dz, but K-FAC uses covariances of dz (output-gradients).\n",
    "        # We can extract dz2 and dz1 from the cache/grads by recomputing backwards.\n",
    "        return activations, errors\n",
    "\n",
    "    def step(self, cache, grads, batch_size):\n",
    "        # Update EMA of A and G\n",
    "        # compute activation covariances A_l = E[a_l a_l^T]\n",
    "        activations, errors = self._extract_activations_and_errors(cache, grads)\n",
    "        # For output grad covariances G we need per-sample pre-activation gradients.\n",
    "        # For simplicity we re-compute per-sample dz arrays in a small-batch manner inside the training loop.\n",
    "        # Here we assume the training loop provides grads computed over the same batch and also provides per-sample dz.\n",
    "        # To keep API simple, we expect cache to contain 'dz1' and 'dz2' as per-sample arrays when available.\n",
    "        assert 'dz1' in cache and 'dz2' in cache, \"cache must contain per-sample dz1 and dz2 for K-FAC\"\n",
    "        A_W1 = compute_covariance(activations['W1'])\n",
    "        A_W2 = compute_covariance(activations['W2'])\n",
    "        G_W2 = compute_covariance(cache['dz2'])  # covariance of output pre-activation gradients (d_out x d_out)\n",
    "        G_W1 = compute_covariance(cache['dz1'])  # covariance of hidden pre-activation gradients (d_hidden x d_hidden)\n",
    "\n",
    "        # EMA update\n",
    "        if 'W1' not in self.A:\n",
    "            self.A['W1'] = A_W1\n",
    "            self.A['W2'] = A_W2\n",
    "            self.G['W1'] = G_W1\n",
    "            self.G['W2'] = G_W2\n",
    "        else:\n",
    "            self.A['W1'] = self.factor_ema * self.A['W1'] + (1 - self.factor_ema) * A_W1\n",
    "            self.A['W2'] = self.factor_ema * self.A['W2'] + (1 - self.factor_ema) * A_W2\n",
    "            self.G['W1'] = self.factor_ema * self.G['W1'] + (1 - self.factor_ema) * G_W1\n",
    "            self.G['W2'] = self.factor_ema * self.G['W2'] + (1 - self.factor_ema) * G_W2\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # invert factors periodically\n",
    "        if self.steps % self.invert_every == 0 or not self.A_inv:\n",
    "            self.A_inv['W1'] = damp_and_invert(self.A['W1'], self.damping)\n",
    "            self.A_inv['W2'] = damp_and_invert(self.A['W2'], self.damping)\n",
    "            self.G_inv['W1'] = damp_and_invert(self.G['W1'], self.damping)\n",
    "            self.G_inv['W2'] = damp_and_invert(self.G['W2'], self.damping)\n",
    "\n",
    "        # Precondition gradients per-layer:\n",
    "        precond = {}\n",
    "        # For W2: grad shape (d_hidden, d_out) -> precondition as A_inv @ grad @ G_inv\n",
    "        precond['W2'] = self.A_inv['W2'].dot(grads['W2']).dot(self.G_inv['W2'])\n",
    "        precond['W1'] = self.A_inv['W1'].dot(grads['W1']).dot(self.G_inv['W1'])\n",
    "        # biases: use diagonal preconditioning (divide by average of diag)\n",
    "        precond['b2'] = grads['b2']  # for simplicity, don't precondition biases strongly\n",
    "        precond['b1'] = grads['b1']\n",
    "\n",
    "        # optionally apply weight decay\n",
    "        if self.weight_decay:\n",
    "            precond['W2'] += self.weight_decay * self.model.params['W2']\n",
    "            precond['W1'] += self.weight_decay * self.model.params['W1']\n",
    "            precond['b2'] += self.weight_decay * self.model.params['b2']\n",
    "            precond['b1'] += self.weight_decay * self.model.params['b1']\n",
    "\n",
    "        # step-size clipping via KL-approx (optional, here we do simple global scaling)\n",
    "        # compute norm of preconditioned step\n",
    "        step_norm = np.sqrt(np.sum(precond['W1']**2) + np.sum(precond['W2']**2))\n",
    "        # simple adaptive global scaling to limit step size\n",
    "        clipped_lr = self.lr\n",
    "        # apply parameter update\n",
    "        self.model.params['W1'] -= clipped_lr * precond['W1']\n",
    "        self.model.params['W2'] -= clipped_lr * precond['W2']\n",
    "        self.model.params['b1'] -= clipped_lr * precond['b1']\n",
    "        self.model.params['b2'] -= clipped_lr * precond['b2']\n",
    "\n",
    "        return precond, step_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9a6d0",
   "metadata": {},
   "source": [
    "### Step-7: Functions for Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29fb2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_per_sample_dz(model, X, Y):\n",
    "    # compute forward and per-sample dz values used by K-FAC\n",
    "    # returns loss, grads (batch-averaged), cache (with per-sample dz1, dz2)\n",
    "    # we will compute per-sample dzs by computing pre-activation outputs for each sample\n",
    "    # and using vectorized formulas\n",
    "    z1 = X.dot(model.params['W1']) + model.params['b1']\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1.dot(model.params['W2']) + model.params['b2']\n",
    "    probs = softmax(z2)\n",
    "    loss = cross_entropy_loss(probs, Y)\n",
    "    N = X.shape[0]\n",
    "    dz2 = (probs - Y)  # shape (N, d_out)  (note: not divided by N here; K-FAC covariances will average)\n",
    "    dW2 = a1.T.dot(dz2) / N\n",
    "    db2 = dz2.sum(axis=0) / N\n",
    "    da1 = dz2.dot(model.params['W2'].T) / N\n",
    "    dz1 = da1 * relu_grad(z1) * N  # multiply by N to get per-sample pre-activation grads (consistent with cov calc)\n",
    "    # dz1 shape (N, d_hidden); dz2 shape (N, d_out)\n",
    "    dW1 = X.T.dot( (dz1 / N) )  # divide back to get averaged grad\n",
    "    db1 = dz1.sum(axis=0) / N\n",
    "    grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'probs': probs, 'dz1': dz1, 'dz2': dz2}\n",
    "    return loss, grads, cache\n",
    "\n",
    "def accuracy(model, X, Y_true):\n",
    "    probs, _ = model.forward(X)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    return (preds == Y_true).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d6076",
   "metadata": {},
   "source": [
    "### Step-8: Training (Single Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ecf287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(optimizer_name='kfac', lr=0.1, damping=1e-3, invert_every=10, batch_size=64, n_epochs=10, print_every=50):\n",
    "    model = SmallMLP(d_hidden=32)\n",
    "    n_samples = X_train.shape[0]\n",
    "    steps_per_epoch = max(1, n_samples // batch_size)\n",
    "    total_steps = n_epochs * steps_per_epoch\n",
    "\n",
    "    # initialize optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        opt = SGDOptimizer(model.params, lr=lr)\n",
    "    elif optimizer_name == 'adam':\n",
    "        opt = AdamOptimizer(model.params, lr=lr)\n",
    "    elif optimizer_name == 'kfac':\n",
    "        kfac = KFACOptimizer(model, lr=lr, damping=damping, invert_every=invert_every)\n",
    "        opt = kfac\n",
    "    else:\n",
    "        raise ValueError(\"unknown optimizer\")\n",
    "\n",
    "    losses = []\n",
    "    vals = []\n",
    "    step_norms = []\n",
    "    # training loop\n",
    "    step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        # simple shuffling\n",
    "        idx = np.random.permutation(n_samples)\n",
    "        for b in range(steps_per_epoch):\n",
    "            batch_idx = idx[b*batch_size:(b+1)*batch_size]\n",
    "            xb = X_train[batch_idx]\n",
    "            yb = Y_train[batch_idx]\n",
    "\n",
    "            # compute loss, grads, cache with per-sample dzs if K-FAC\n",
    "            loss, grads, cache = forward_with_per_sample_dz(model, xb, yb)\n",
    "            losses.append(loss)\n",
    "            # step\n",
    "            if optimizer_name in ('sgd', 'adam'):\n",
    "                opt.step(grads)\n",
    "                # approximate step norm as norm of parameter change (we don't track exact change here)\n",
    "                step_norms.append(np.sqrt(np.sum((lr*grads['W1'])**2) + np.sum((lr*grads['W2'])**2)))\n",
    "            else:  # K-FAC\n",
    "                precond, s_norm = opt.step(cache, grads, batch_size)\n",
    "                step_norms.append(s_norm)\n",
    "\n",
    "            # validation metric every so often\n",
    "            if step % print_every == 0:\n",
    "                val_acc = accuracy(model, X_val, y[split:])\n",
    "                vals.append((step, val_acc))\n",
    "            step += 1\n",
    "\n",
    "    # final validation accuracy\n",
    "    final_acc = accuracy(model, X_val, y[split:])\n",
    "    return {'losses': np.array(losses), 'val_records': vals, 'step_norms': np.array(step_norms), 'final_acc': final_acc, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7976655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD final val acc: 0.9875\n",
      "Adam final val acc: 0.9875\n",
      "K-FAC final val acc: 0.5875\n"
     ]
    }
   ],
   "source": [
    "# quick smoke-run to ensure things work\n",
    "res_sgd = train_model(optimizer_name='sgd', lr=0.2, n_epochs=5, batch_size=64, print_every=200)\n",
    "res_adam = train_model(optimizer_name='adam', lr=0.01, n_epochs=5, batch_size=64, print_every=200)\n",
    "res_kfac = train_model(optimizer_name='kfac', lr=0.5, damping=1e-2, invert_every=5, n_epochs=5, batch_size=64, print_every=200)\n",
    "print(\"SGD final val acc:\", res_sgd['final_acc'])\n",
    "print(\"Adam final val acc:\", res_adam['final_acc'])\n",
    "print(\"K-FAC final val acc:\", res_kfac['final_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7dbb83",
   "metadata": {},
   "source": [
    "### Step-9: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5d9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_dict, title_suffix=\"\"):\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    for name, res in results_dict.items():\n",
    "        plt.plot(res['losses'], label=name)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Training loss (log scale) \" + title_suffix)\n",
    "    plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.legend(); plt.grid(ls='--', lw=0.3)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    for name, res in results_dict.items():\n",
    "        sn = res['step_norms']\n",
    "        plt.plot(sn, label=name)\n",
    "    plt.title(\"Step norms per step\")\n",
    "    plt.xlabel(\"step\"); plt.ylabel(\"step norm\"); plt.legend(); plt.grid(ls='--', lw=0.3)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    for name, res in results_dict.items():\n",
    "        val_rec = res['val_records']\n",
    "        if val_rec:\n",
    "            xs = [v[0] for v in val_rec]\n",
    "            ys = [v[1] for v in val_rec]\n",
    "            plt.plot(xs, ys, marker='o', label=name)\n",
    "    plt.title(\"Validation accuracy checkpoints\")\n",
    "    plt.xlabel(\"step\"); plt.ylabel(\"val acc\"); plt.legend(); plt.grid(ls='--', lw=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e3a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735d8d453f0149dc9d96c84b48c77de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.2, description='lr SGD', max=1.0, min=0.01, step=0.01), FloatSlider(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if widgets is None:\n",
    "    display(Markdown(\"**ipywidgets not installed â€” run a static experiment instead.**\"))\n",
    "else:\n",
    "    def interactive_compare(lr_sgd=0.2, lr_adam=0.01, lr_kfac=0.5, damping=1e-2, invert_every=5, n_epochs=8, batch_size=64):\n",
    "        clear_output(wait=True)\n",
    "        display(Markdown(f\"### Running experiments: n_epochs={n_epochs}, batch_size={batch_size}\"))\n",
    "        # run three experiments sequentially (small models are cheap)\n",
    "        res_sgd = train_model(optimizer_name='sgd', lr=lr_sgd, n_epochs=n_epochs, batch_size=batch_size, print_every=200)\n",
    "        display(Markdown(f\"SGD final val acc: **{res_sgd['final_acc']:.3f}**\"))\n",
    "        res_adam = train_model(optimizer_name='adam', lr=lr_adam, n_epochs=n_epochs, batch_size=batch_size, print_every=200)\n",
    "        display(Markdown(f\"Adam final val acc: **{res_adam['final_acc']:.3f}**\"))\n",
    "        res_kfac = train_model(optimizer_name='kfac', lr=lr_kfac, damping=damping, invert_every=invert_every, n_epochs=n_epochs, batch_size=batch_size, print_every=200)\n",
    "        display(Markdown(f\"K-FAC final val acc: **{res_kfac['final_acc']:.3f}**\"))\n",
    "        results = {'SGD': res_sgd, 'Adam': res_adam, 'K-FAC': res_kfac}\n",
    "        plot_results(results, title_suffix=f\"(epochs={n_epochs})\")\n",
    "    interact(\n",
    "        interactive_compare,\n",
    "        lr_sgd=FloatSlider(value=0.2, min=0.01, max=1.0, step=0.01, description='lr SGD'),\n",
    "        lr_adam=FloatSlider(value=0.01, min=0.001, max=0.1, step=0.001, description='lr Adam'),\n",
    "        lr_kfac=FloatSlider(value=0.5, min=0.01, max=2.0, step=0.01, description='lr K-FAC'),\n",
    "        damping=FloatSlider(value=1e-2, min=1e-6, max=1e-1, step=1e-5, description='damping'),\n",
    "        invert_every=IntSlider(value=5, min=1, max=20, step=1, description='inv every'),\n",
    "        n_epochs=IntSlider(value=8, min=1, max=30, step=1, description='epochs'),\n",
    "        batch_size=IntSlider(value=64, min=8, max=200, step=8, description='batch')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a47ab",
   "metadata": {},
   "source": [
    "### Step-10: Parameter subspace trajectory projection (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6530b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_trajectory_pca(res_dict, keys=('W1', 'W2')):\n",
    "    # Collect flattened parameter vectors across steps for each run and project to 2D via PCA\n",
    "    trajectories = {}\n",
    "    for name, res in res_dict.items():\n",
    "        model = res['model']\n",
    "        # reconstruct snapshots are not saved per step in this simple setup.\n",
    "        # As a proxy we will re-train while recording parameters each step (cheap for small model) â€” but to keep time low we sample fewer steps.\n",
    "        # Here we will re-run a short trace to collect parameter snapshots (not ideal but clear).\n",
    "        model = SmallMLP(d_hidden=32)\n",
    "        snapshots = []\n",
    "        n_samples = X_train.shape[0]\n",
    "        batch_size = 64\n",
    "        steps_per_epoch = max(1, n_samples // batch_size)\n",
    "        total_steps = 50\n",
    "        idx = np.random.permutation(n_samples)\n",
    "        step = 0\n",
    "        while step < total_steps:\n",
    "            batch_idx = idx[(step*batch_size)%n_samples:((step+1)*batch_size)%n_samples]\n",
    "            xb = X_train[batch_idx]\n",
    "            yb = Y_train[batch_idx]\n",
    "            loss, grads, cache = forward_with_per_sample_dz(model, xb, yb)\n",
    "            if name == 'SGD':\n",
    "                SGDOptimizer(model.params, lr=0.2).step(grads)\n",
    "            elif name == 'Adam':\n",
    "                AdamOptimizer(model.params, lr=0.01).step(grads)\n",
    "            else:\n",
    "                kfac = KFACOptimizer(model, lr=0.5, damping=1e-2, invert_every=5)\n",
    "                kfac.step(cache, grads, batch_size)\n",
    "            vec = np.concatenate([model.params['W1'].ravel(), model.params['W2'].ravel()])\n",
    "            snapshots.append(vec)\n",
    "            step += 1\n",
    "        trajectories[name] = np.array(snapshots)\n",
    "    # stack all trajectories to fit PCA\n",
    "    all_vecs = np.vstack([v for v in trajectories.values()])\n",
    "    # mean center\n",
    "    mean = all_vecs.mean(axis=0)\n",
    "    V = all_vecs - mean\n",
    "    # simple PCA via SVD\n",
    "    U, S, VT = np.linalg.svd(V.T, full_matrices=False)\n",
    "    top2 = VT[:2].T  # basis vectors\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    for name, traj in trajectories.items():\n",
    "        proj = (traj - mean).dot(top2)\n",
    "        plt.plot(proj[:,0], proj[:,1], '-o', label=name)\n",
    "    plt.title(\"Parameter trajectory projections (PCA of sampled params)\")\n",
    "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.legend(); plt.grid(ls='--', lw=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d211d6",
   "metadata": {},
   "source": [
    "## Next steps and caveats\n",
    "\n",
    "- The K-FAC above uses small-batch empirical covariances and dense inverses â€” in practice:\n",
    "  - Use layerwise damping scheduled adaptively.\n",
    "  - Use running-averages (EMA) with a reasonable decay (we used `factor_ema`).\n",
    "  - For convolutional layers, K-FAC uses structured Kronecker factors (not shown here).\n",
    "  - Real implementations are vectorized and integrated into autograd frameworks (PyTorch/TensorFlow) for efficiency.\n",
    "\n",
    "- This notebook is intentionally explicit: it trades some performance for clarity so you can see the algebraic steps:\n",
    "  - factor = E[a a^T]\n",
    "  - factor2 = E[d a d a^T]\n",
    "  - precondition gradient by factor^{-1} on both sides.\n",
    "\n",
    "**Reference**\n",
    "- Martens, J., & Grosse, R. (2015). *Optimizing Neural Networks with Kronecker-factored Approximate Curvature (K-FAC).* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45193d6f",
   "metadata": {},
   "source": [
    "## Notes, performance & practical tips\n",
    "\n",
    "- The NumPy implementation is intentionally simple and educational. On larger models and real datasets:\n",
    "    - Use minibatch sizes large enough to estimate factors accurately.\n",
    "    - Use small damping (Levenbergâ€“Marquardt style) to stabilize inverses.\n",
    "    - Update factor inverses less frequently to save compute.\n",
    "    - Combine K-FAC with momentum/Adam-style schedules in practice (there are many heuristics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0605c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
