# ğŸ§  Optimizers for Machine Learning  
### *A Mathematical, Visual, and Conceptual Exploration*

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/1*OE4zIhX9IhyQ9iE6Z0pO9A.gif" width="600" alt="Optimization Landscape">
</p>

---

## ğŸ“˜ Overview

This repository provides an **in-depth, postgraduate-level exploration of optimization algorithms** used in machine learning â€” from **classical gradient-based methods** to **state-of-the-art curvature-aware optimizers**.

Each notebook blends:
- ğŸ“ **Rigorous mathematical foundations**
- ğŸ“ˆ **Interactive visualizations**
- ğŸ§© **Geometric intuitions**
- ğŸ’¡ **Comparative experiments**

All explanations are self-contained and progressively build intuition for the underlying geometry, convergence behavior, and practical trade-offs of each method.

---

## ğŸ§­ Repository Structure

```
optimizers-ml/
â”‚
â”œâ”€â”€ 01_basics/                                  # Foundational Optimization Methods
â”‚   â”œâ”€â”€ gradient_descent.ipynb                  # Standard Gradient Descent (GD) and Stochastic Gradient Descent (SGD).
â”‚   â”œâ”€â”€ momentum_and_nesterov.ipynb             # Momentum-based acceleration: Classical Momentum and
|                                                 Nesterov Accelerated Gradient (NAG).
â”‚   â”œâ”€â”€ adaptive_methods_adagrad_adam.ipynb     # Adaptive learning rate methods: AdaGrad, RMSprop, and Adam.
â”‚
â”œâ”€â”€ 02_sota_optimizers/                         # State-of-the-Art (SOTA) Optimizers
â”‚   â”œâ”€â”€ adamw_and_decoupled_weight_decay.ipynb  # AdamW and the principle of decoupled weight decay.
â”‚   â”œâ”€â”€ ranger_lookahead_radam.ipynb            # RAdam (Rectified Adam), Lookahead, and Ranger.
â”‚   â”œâ”€â”€ lion_and_adamx.ipynb                    # Modern, high-performance optimizers like Lion and AdamX.
â”‚   
â”œâ”€â”€ 03_curvature_aware/                     # Second-Order and Curvature-Aware Methods
â”‚   â”œâ”€â”€ natural_gradient_descent.ipynb      # Theory and implementation of Natural Gradient Descent (NGD).
â”‚   â”œâ”€â”€ kfac_and_shampoo.ipynb              # Scalable curvature methods: K-FAC and Shampoo.
â”‚   â”œâ”€â”€ adaptive_curvature_methods.ipynb    # Study of other methods leveraging curvature information.
â”‚
â”œâ”€â”€ 04_visualizations/                      # Tools and Analysis for Optimization Landscapes
â”‚   â”œâ”€â”€ optimization_landscapes.ipynb       # Generating 2D and 3D optimization loss landscapes.
â”‚   â”œâ”€â”€ loss_surface_geometry.ipynb         # Analysis of loss surface geometry (e.g., Hessian eigenvalues).
â”‚   â”œâ”€â”€ convergence_animations.ipynb        # Generating animated visualizations of optimizer convergence paths.
â”‚
â”œâ”€â”€ assets/                                 # Supporting media for notebooks
â”‚   â”œâ”€â”€ figures/                            # Static images and plots.
â”‚   â”œâ”€â”€ gifs/                               # Animated convergence visualizations.
â”‚
â”œâ”€â”€ requirements.txt                      # List of Python dependencies.
â””â”€â”€ README.md                             # The main project description file.
```

---

## ğŸ”¬ Topics Covered

| Module | Theme | Highlights |
|--------|--------|-------------|
| **01. Gradient-Based Foundations** | Understanding how optimization connects calculus, linear algebra, and geometry. | Gradient Descent, Momentum, Nesterov Accelerated Gradient |
| **02. State-of-the-Art Optimizers** | Modern optimizers that dominate deep learning pipelines. | Adam, AdamW, RAdam, Lookahead, LION |
| **03. Curvature-Aware Methods** | Second-order and quasi-second-order optimizers using curvature information. | Natural Gradient, K-FAC, Shampoo, AdaHessian |
| **04. Visualization & Geometry** | Visual intuition of loss surfaces and optimizer dynamics. | 2D/3D plots, contour visualizations, optimization path animations |

---

## ğŸ§® Mathematical Depth

Each notebook includes:
- Derivations of update rules  
- Convergence analysis sketches  
- Theoretical connections to optimization theory and information geometry  
- Practical considerations: bias correction, numerical stability, and scaling  

Example snippet from *Natural Gradient Descent*:

```math
Î¸_{t+1} = Î¸_t - Î· F^{-1}(Î¸_t) âˆ‡_Î¸ L(Î¸_t)
```

where F(Î¸) is the Fisher Information Matrix, connecting optimization to information geometry.

---

## ğŸ¨ Visualization Examples

- Optimization Trajectories over non-convex surfaces
- Learning rate schedules and their effects
- Curvature fields and metric tensors
- Animated convergence paths

<p align="center"> <img src="https://raw.githubusercontent.com/username/optimizers-ml/assets/figures/optimizer_trajectories.gif" width="600" alt="Optimizer Trajectories"> </p>

---

## ğŸ§  Learning Outcomes

- After working through this repository, you will:
- Understand the mathematical principles of modern optimizers
- Gain geometric intuition about curvature, conditioning, and step-size adaptation
- Appreciate the trade-offs between efficiency and generalization
- Be able to prototype and visualize optimizers in PyTorch / JAX

---

## ğŸ§° Tech Stack

- Python 3.11+
- Jupyter Notebooks
- NumPy, Matplotlib, Plotly, SymPy
- PyTorch or JAX for experiments
- Manim or matplotlib.animation for geometric visualizations

---

## Install dependencies:

```python
pip install -r requirements.txt
```

---

## ğŸ“Š Example Visualization

### Visualize optimizer trajectories over a 3D loss surface

```python
from visualization import plot_optimizer_path
plot_optimizer_path(loss_fn="rosenbrock", optimizers=["GD", "Adam", "NGD"])
```

---

## ğŸ§© Suggested Reading
- Goodfellow, Bengio, Courville (2016): Deep Learning â€” Chapter 8
- Martens & Grosse (2015): Optimizing Neural Networks with Kronecker-Factored Approximate Curvature
- Kingma & Ba (2015): Adam: A Method for Stochastic Optimization
- Zhang et al. (2022): The LION Optimizer
- Pascanu & Bengio (2014): Revisiting Natural Gradient Methods

---

## ğŸš€ Roadmap
 - Foundations and classical methods
 - SOTA adaptive optimizers
 - Curvature-aware optimizers (in progress)
 - Interactive dashboard (Streamlit) for optimizer comparison
 - Paper summaries and geometric notes

---

## ğŸ§­ Contributing

Contributions are welcome!
If you have new visualizations, mathematical insights, or corrections:
Fork the repository
Create a new branch
Submit a pull request with a clear description

---

## ğŸ§‘â€ğŸ« Author

Agasthya 

Researcher in Optimization Theory

---

## ğŸ§¾ License
This repository is licensed under the MIT License.
Feel free to use, modify, and cite for academic and educational purposes.

---

## ğŸŒŸ Acknowledgements

Special thanks to the open-source and academic communities advancing our understanding of optimization theory and practice.

**â€œOptimization is not merely about descent â€” itâ€™s about navigating the geometry of intelligence.â€**